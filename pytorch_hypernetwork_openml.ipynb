{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "#from torchvision.datasets import MNIST\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "#import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#from torchvision.transforms import ToTensor\n",
    "#from torchvision.utils import make_grid\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.init\n",
    "from torch.nn.modules import Module\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import f1_score\n",
    "# Use a white background for matplotlib figures\n",
    "matplotlib.rcParams['figure.facecolor'] = '#ffffff'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import openml\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(outputs, labels):    \n",
    "    labels_onehot = torch.nn.functional.one_hot(labels, num_classes=2).cpu().detach().numpy()\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    #print(outputs.shape)\n",
    "    outputs = outputs.cpu().detach().numpy()\n",
    "    outputs = outputs.reshape(outputs.shape[0]*outputs.shape[1],outputs.shape[2])\n",
    "    labels_onehot = labels_onehot.reshape(labels_onehot.shape[0]*labels_onehot.shape[1],labels_onehot.shape[2])\n",
    "    #acc = torch.tensor(torch.sum(preds == labels).item() / len(preds)).mean()\n",
    "    auc = np.round(roc_auc_score(labels_onehot,outputs),4)\n",
    "    mcc=0\n",
    "    f=0\n",
    "    #mcc = matthews_corrcoef(labels,preds)\n",
    "    #f = f1_score(labels,preds, average='weighted')\n",
    "    return auc, mcc, f\n",
    "\n",
    "#Define accuracy metric\n",
    "def accuracy(outputs, labels):\n",
    "    labels_onehot = torch.nn.functional.one_hot(labels, num_classes=10).cpu().detach().numpy()\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    acc = torch.tensor(np.round(torch.sum(preds == labels).item() / len(preds),4))\n",
    "    #global_outputs.append(outputs.detach().numpy())\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperNet(nn.Module):\n",
    "    \n",
    "    __constants__ = ['in_features', 'out_features']\n",
    "    in_features: int\n",
    "    out_features: int\n",
    "    weight1: Tensor\n",
    "    bias1: Tensor\n",
    "    bias2: Tensor\n",
    "    weight2: Tensor\n",
    "    zval: Tensor\n",
    "        \n",
    "    def __init__(self, in_features: int, out_features: int, z_dim: int, bias: bool = False,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(HyperNet, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.z_dim = z_dim\n",
    "        self.weight1 = nn.Parameter(torch.fmod(torch.randn((in_features,z_dim),**factory_kwargs),2))\n",
    "        self.weight2 = nn.Parameter(torch.fmod(torch.randn((z_dim,out_features),**factory_kwargs),2))\n",
    "        if bias:\n",
    "            self.bias1 = nn.Parameter(torch.fmod(torch.randn((z_dim), **factory_kwargs),2))\n",
    "            self.bias2 = nn.Parameter(torch.fmod(torch.randn((out_features), **factory_kwargs),2))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = torch.matmul(xb,self.weight1) + self.bias1\n",
    "        # NO activation required\n",
    "        # output of hypernet layer2\n",
    "        xb = torch.matmul(xb,self.weight2) + self.bias2\n",
    "        #return weights for main network\n",
    "        return xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define model\n",
    "class MnistModel(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size, out_size, z_dim):\n",
    "        super().__init__()\n",
    "        # hidden layer\n",
    "        self.linear1 = HyperNet(in_size, hidden_size, z_dim, bias=True)\n",
    "        # output layer\n",
    "        self.linear2 = HyperNet(hidden_size, out_size, z_dim, bias=True)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        # Flatten the image tensors\n",
    "        ##xb = (xb.view(xb.size(0), -1)).clone().detach()\n",
    "        # Get intermediate outputs using hidden layer\n",
    "        out = self.linear1(xb)\n",
    "        # Apply activation function\n",
    "        out = F.tanh(out)\n",
    "        # Get predictions using output layer\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)\n",
    "        # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'loss': loss, 'acc': acc, 'out': out, 'labels': labels}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        batch_out = [x['out'] for x in outputs]\n",
    "        epoch_out = torch.stack(batch_out)\n",
    "        batch_labels = [x['labels'] for x in outputs]\n",
    "        epoch_labels = torch.stack(batch_labels)\n",
    "        epoch_auc, mcc, f = calculate_metrics(epoch_out, epoch_labels)\n",
    "        return {'epoch_loss': epoch_loss.item(), 'epoch_acc': epoch_acc.item(), 'epoch_auc': epoch_auc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        return\n",
    "        #print(\"Epoch [{}], loss: {:.4f}, acc: {:.4f}, auc score: {:.4f}\".format(epoch, result['epoch_loss'],result['epoch_acc'],result['epoch_auc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training and validation indices\n",
    "def split_indices(n, val_pct):\n",
    "    n_val = int(val_pct*n)\n",
    "    idxs = np.random.permutation(n)\n",
    "    return idxs[n_val:], idxs[:n_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Move to cuda if available\n",
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    else:\n",
    "        return data.to(device, non_blocking=True)\n",
    "    \n",
    "class DeviceDataLoader():\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define fit function\n",
    "def evaluate(model, loader):\n",
    "    \"\"\"Evaluate the model's performance on the validation set\"\"\"\n",
    "    outputs=[]\n",
    "    for batch in loader:\n",
    "        outputs.append(model.validation_step(batch))\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, test_loader, opt_func=torch.optim.SGD):\n",
    "    \"\"\"Train the model using gradient descent\"\"\"\n",
    "    history = []\n",
    "    last_loss=0\n",
    "    error_queue = []\n",
    "    avg_error=1\n",
    "    last_iter=0\n",
    "    #loss_threshold=0.37\n",
    "    row['Learning Rate'] = lr\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        last_iter = epoch + 1\n",
    "        total_loss=0\n",
    "        num_batches=0\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            \n",
    "            total_loss = total_loss + loss\n",
    "            num_batches = num_batches+1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        avg_loss = total_loss/num_batches\n",
    "        change = abs(last_loss - avg_loss)\n",
    "        #if loss < loss_threshold:\n",
    "        if len(error_queue) < 20:\n",
    "            error_queue.append(abs(change))\n",
    "            #print(error_queue)\n",
    "        else:\n",
    "            error_queue.append(abs(change))\n",
    "            #print(error_queue)\n",
    "            avg_error = sum(error_queue[1:21])/len(error_queue[1:21])\n",
    "            error_queue.pop(0)\n",
    "\n",
    "            #print(avg_error)       \n",
    "            if avg_error <= 0.00000001: #Error threshold based early stopping\n",
    "                break\n",
    "\n",
    "        last_loss = loss\n",
    "            \n",
    "    row[\"Iter/Generation\"] = last_iter\n",
    "    # Training phase\n",
    "    result = evaluate(model, train_loader)\n",
    "    model.epoch_end(epochs, result)\n",
    "    history.append(result.copy())\n",
    "    \n",
    "    # Validation phase\n",
    "    result = evaluate(model, val_loader)\n",
    "    model.epoch_end(epochs, result)\n",
    "    history.append(result.copy())\n",
    "    \n",
    "    # Testing Phase\n",
    "    result = evaluate(model, test_loader)\n",
    "    model.epoch_end(epochs, result)\n",
    "    history.append(result.copy())\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, annotations_file, transform=None, target_transform=None):\n",
    "        #df = pd.read_csv(annotations_file)\n",
    "        df = annotations_file\n",
    "        self.labels = df['label']\n",
    "        input_size = len(df.columns)-1\n",
    "        self.data = df.iloc[:,0:input_size]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data.values\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        x_scaled = np.round(min_max_scaler.fit_transform(x), 3)\n",
    "        self.data = pd.DataFrame(x_scaled)\n",
    "        \n",
    "        label_read = torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "        data_read = torch.tensor(self.data.iloc[idx], dtype=torch.float32)\n",
    "        return data_read, label_read\n",
    "\n",
    "\n",
    "def get_data(Dataset):\n",
    "    batch_size=25\n",
    "    device = get_default_device()\n",
    "    #num_classes=2\n",
    "    dataset = CustomDataset(Dataset)\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [0.6,0.2,0.2], generator=torch.Generator())\n",
    "    train_loader = DataLoader(train_dataset, batch_size,drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size,drop_last=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size,drop_last=True)\n",
    "    \n",
    "    train_loader = DeviceDataLoader(train_loader, device)\n",
    "    val_loader = DeviceDataLoader(val_loader, device)\n",
    "    test_loader = DeviceDataLoader(test_loader, device)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1173698572.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[19], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    '''Date Input/Output code for CSV dataset'''\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "def get_data(Dataset):\n",
    "    batch_size=25\n",
    "    '''Date Input/Output code for CSV dataset'''\n",
    "   '''\n",
    "    device = get_default_device()\n",
    "    #df = pd.read_csv('../Dataset/pneumonia/pneumonia_train.csv')\n",
    "    df = pd.read_csv('../Dataset/Wisconsin_Preprocessed.csv')\n",
    "    #print(df.shape)\n",
    "    \n",
    "    num_classes=2\n",
    "    # Reading the input data.\n",
    "    dataset = torch.Tensor(np.array(df))\n",
    "    input_size = len(df.columns)-1\n",
    "    df = df.iloc[:,0:input_size]\n",
    "    dataset = CustomDataset(\"../Dataset/Wisconsin_Preprocessed.csv\")\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [0.6,0.2,0.2], generator=torch.Generator())\n",
    "    train_loader = DataLoader(train_dataset, batch_size,drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size,drop_last=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size,drop_last=True)\n",
    "    \n",
    "    train_loader = DeviceDataLoader(train_loader, device)\n",
    "    val_loader = DeviceDataLoader(val_loader, device)\n",
    "    test_loader = DeviceDataLoader(test_loader, device)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def save_to_csv(df, name):\n",
    "    path = 'Output/' + name\n",
    "    if not os.path.exists('Output'):\n",
    "        os.makedirs('Output')\n",
    "    df.to_csv(path, index=False)\n",
    "\n",
    "def create_csv():\n",
    "    columns = ['Dataset', 'Instances','Features', 'Hidden Layer', 'Output Layer',\n",
    "               'Iter/Generation', 'Input Size', \n",
    "               'Learning Rate', 'Train Accuracy',\n",
    "               'Train AUC', 'Val Accuracy',\n",
    "               'Val AUC', 'Test Accuracy',\n",
    "               'Test AUC', 'Time Taken(s)',\n",
    "               'Time per generation', 'Memory Required', 'Total Memory(kB)',\n",
    "               'Algorithmic Complexity']\n",
    "    \n",
    "    return pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "803\n",
      "[False, False, False, False, False]\n",
      "[0, 1, 2, 3, 4]\n",
      "features=5\n",
      "rep= 0\n",
      "rep= 1\n",
      "rep= 2\n",
      "rep= 3\n",
      "rep= 4\n",
      "1496\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "features=20\n",
      "rep= 0\n",
      "rep= 1\n",
      "rep= 2\n",
      "rep= 3\n",
      "rep= 4\n",
      "1507\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "features=20\n",
      "rep= 0\n",
      "rep= 1\n",
      "rep= 2\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def run(layer, train_loader, val_loader, test_loader):\n",
    "    global df, input_size\n",
    "    #print(hidden_size)\n",
    "    z_dim=hidden_size/2\n",
    "    start = time.time()\n",
    "    #result = [{'epoch_acc': 1, 'epoch_auc': 2}, {'epoch_acc': 1, 'epoch_auc': 2}, {'epoch_acc': 1, 'epoch_auc': 2}]\n",
    "    result = fit(500, 0.01, model, train_loader, val_loader, test_loader)\n",
    "    row['Time Taken(s)'] = time.time() - start\n",
    "    row['Time per generation'] = row['Time Taken(s)'] / row['Iter/Generation']\n",
    "    row['Input Size'] = input_size\n",
    "    #row['Num Classes'] = num_classes\n",
    "    #row['Z Dim'] = z_dim\n",
    "\n",
    "    row['Train Accuracy'] = result[0]['epoch_acc']\n",
    "    row['Train AUC'] = result[0]['epoch_auc']\n",
    "    row['Val Accuracy'] = result[1]['epoch_acc']\n",
    "    row['Val AUC'] = result[1]['epoch_auc']\n",
    "    row['Test Accuracy'] = result[2]['epoch_acc']\n",
    "    row['Test AUC'] = result[2]['epoch_auc']\n",
    "    \n",
    "    memory_req = (input_size + hidden_size + num_classes) * z_dim\n",
    "    row['Memory Required'] = memory_req\n",
    "    row['Total Memory(kB)'] = memory_req/1024.0\n",
    "    row['Algorithmic Complexity'] = (memory_req * row['Iter/Generation'])/10000.0\n",
    "    row['Output Layer'] = num_classes\n",
    "    row['Hidden Layer'] = layer\n",
    "\n",
    "    df1 = pd.DataFrame(row, index=[0])\n",
    "    df = pd.concat([df, df1], ignore_index=True)\n",
    "\n",
    "df = create_csv()\n",
    "if __name__ == '__main__':\n",
    "    input_size = 0\n",
    "    num_classes = 0\n",
    "    repeat = 5\n",
    "    layers = [10, 20, 50, 100, 200]\n",
    "    #layers= [10] 472, 946 736,782,\n",
    "    #small datasets 1\n",
    "    #dataset_final = [714]\n",
    "    #dataset_final = [924,885,974,784,902,969,955,748,801,446,733,796,996,1005,895,464,450,1073,880,1121,44149,40710,43,818,1524,915,1167,925,1011,1048,860,900,906,907,908,909,1511,1498,724,814,4329,750,886,987,825,717,1063,1467,947,949,950,951,41945,770,997,774,795,827,931,40981,1452,1464,37,994,841,40705,1016,31,1547,43255,741,1444,1494,45077,1453,1068,1462,1049,983,1050,1504]\n",
    "    #small datasets 2         - run in server - 1116, 40666,\n",
    "    dataset_final = [803,1496,1507,43946,725,735,752,761,807,816,833,923,1056,819,45553,976]\n",
    "    #dataset_final = [962,971,995,1020,914,1067,772,40704,958,40999,41007,316,1487,43980,45562,41143,737,41005,40713,41946,871,728,41156,720,44,40983,43892,43949,979,40701,43947,40900,41146,43958,42192,1460,1489,1021,1069,980,847,1116,40666,803,1496,1507,43946,725,735,752,761,807,816,833,923,1056,819,45553,976]\n",
    "    #dataset_final = 1504,1507,1511,1524,1547,1597,4154,4329,23517,40666,40701,40704,40705,40710,40713,40900,40922,40981,40983,40999,41005,41007,41143,41146,41150,41156,41228,41945,41946,42192,42397,42477,42680,42769,43255,43551,43890,43892,43946,43947,43949,43958,43977,43979,43980,44038,44089,44149,44162,45023,45035,45037,45060,45077,45547,45549,45553,45556,45558,45562,45578]\n",
    "    #dataset_final = [31,37,43,44,151,161,162,246,251,257,260,264,267,269,310,316,446,450,464,472,714,717,720,722,724,725,728,733,734,735,736,737,741,748,750,752,761,770,772,774,782,784,795,796,801,803,807,814,816,818,819,821,823,825,827,833,841,843,846,847,860,871,880,881,885,886,895,900,901,902,906,907,908,909,914,915,923,924,925,931,946,947,949,950,951,955,958,962,969,971,974,976,977,979,980,983,987,994,995,996,997,1005,1011,1016,1019,1020,1021,1040,1046,1048,1049,1050,1056,1063,1067,1068,1069,1073,1116,1120,1121,1167,1169,1182,1205,1212,1216,1444,1452,1453,1460,1461,1462,1464,1467,1471,1486,1487,1489,1494,1496,1498,1502,]\n",
    "    #hidden_size = layers\n",
    "\n",
    "    for did in dataset_final:\n",
    "        row = dict()    \n",
    "        row['Dataset'] = did\n",
    "        print(did)\n",
    "        dataset = openml.datasets.get_dataset(did)\n",
    "    \n",
    "        #get data from the dataset\n",
    "        X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "            dataset_format=\"dataframe\", target=dataset.default_target_attribute\n",
    "        )\n",
    "        \n",
    "        print(categorical_indicator)\n",
    "        i_list=[]\n",
    "        i=0\n",
    "        for ci in categorical_indicator:\n",
    "            if ci == False:\n",
    "                i_list.append(i)\n",
    "            i=i+1\n",
    "        print(i_list)\n",
    "    \n",
    "        X=X.iloc[:,i_list].astype(\"float\")\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "        num_features = len(X.T)\n",
    "        X['label']=y.T\n",
    "        print(\"features=\"+str(num_features))\n",
    "        max_samples = int(len(X))\n",
    "        if max_samples >10000:\n",
    "            max_samples = 10000\n",
    "            X.sample(n=max_samples, random_state=1)\n",
    "        row['Instances'] = max_samples\n",
    "        row['Features'] = num_features\n",
    "        for rep in range(repeat):       \n",
    "            print(\"rep=\",rep)\n",
    "            train_loader, val_loader, test_loader = get_data(X)\n",
    "            for layer in layers:\n",
    "                for batch in train_loader:\n",
    "                    data, labels = batch\n",
    "                    input_size = len(data.T)\n",
    "                    break\n",
    "                num_classes = 2\n",
    "                hidden_size=layer\n",
    "                model = MnistModel(input_size, hidden_size=layer, out_size=num_classes, z_dim=5)\n",
    "                model = model.to(device=get_default_device())\n",
    "                run(layer, train_loader, val_loader, test_loader)\n",
    "            \n",
    "            #display(df)\n",
    "            \n",
    "save_to_csv(df, f'HN_{time.time()}.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'save_to_csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m save_to_csv(df, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHN_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'save_to_csv' is not defined"
     ]
    }
   ],
   "source": [
    "save_to_csv(df, f'HN_{time.time()}.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
